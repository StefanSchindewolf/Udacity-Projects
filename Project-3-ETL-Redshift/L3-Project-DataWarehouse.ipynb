{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3-Sample-Loader\n",
    "import boto3\n",
    "import configparser\n",
    "import os\n",
    "\n",
    "### Check S3 storage for a partial set of the data, download JSON files and load them into DB\n",
    "print(\"DATA SAMPLE LOAD ....\")\n",
    "\n",
    "# Some configurations and initialisations\n",
    "numfil = 10 # number of sample files to download\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dwh.cfg')\n",
    "KEY=config.get('AWS','KEY')\n",
    "SECRET= config.get('AWS','SECRET')\n",
    "BUCKET = config.get('S3', 'S3_BUCKET')\n",
    "songdatafiles = list(())\n",
    "eventfiles = list(())\n",
    "\n",
    "# Connect to S3\n",
    "print(\"Setting up S3 connection for \", BUCKET)\n",
    "s3 = boto3.resource('s3',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                     )\n",
    "projectBucket =  s3.Bucket(BUCKET)\n",
    "\n",
    "# Check S3 for song data using \"song-data\" as prefix\n",
    "print(\"Searching song data\")\n",
    "for songfile in projectBucket.objects.filter(Prefix=\"song-data\"):\n",
    "    if \"json\" in songfile.key:\n",
    "        songdatafiles.append(songfile.key)\n",
    "print(\"Finished checking, found \", len(songdatafiles), \" files\")\n",
    "\n",
    "# Check S3 for events using \"log_data\" as prefi\n",
    "print(\"Searching log events data\")\n",
    "for eventfile in projectBucket.objects.filter(Prefix=\"log_data\"):\n",
    "    if \"json\" in eventfile.key:\n",
    "        eventfiles.append(eventfile.key)\n",
    "print(\"Finished checking, found \", len(eventfiles), \" files\")\n",
    "\n",
    "# Just use the first x files and download them locally\n",
    "partialsongdata = songdatafiles[0:(numfil-1)]\n",
    "partialevents = eventfiles[0:(numfil-1)]\n",
    "\n",
    "# Download song data\n",
    "print(\"Starting download:\")\n",
    "for key in partialsongdata:\n",
    "    fullpath = ('s3://udacity-dend/' + key)\n",
    "    splitted_path = fullpath.split(\"/\")\n",
    "    filename = str(splitted_path[-1])\n",
    "    location = \"tmp/\" + filename\n",
    "    print(\"Downloading ... \", fullpath, \" to \", os.getcwd())\n",
    "    with open(location, 'wb') as file:\n",
    "        s3.Bucket(\"udacity-dend\").download_file(key, location)\n",
    "print(\"Downloaded \", len(partialsongdata), \" files\")\n",
    "\n",
    "# Download event data\n",
    "print(\"Checking for event data on S3 storage...\")\n",
    "print(\"Starting download:\")\n",
    "for key in partialevents:\n",
    "    fullpath = ('s3://udacity-dend/' + key)\n",
    "    splitted_path = fullpath.split(\"/\")\n",
    "    filename = str(splitted_path[-1])\n",
    "    location = \"tmp/\" + filename\n",
    "    print(\"Downloading ... \", fullpath, \" to \", os.getcwd())\n",
    "    with open(location, 'wb') as file:\n",
    "        s3.Bucket(\"udacity-dend\").download_file(key, location)\n",
    "   \n",
    "print(\"Downloaded \", len(partialsongdata), \" files\")\n",
    "print(\"DATA SAMPLE LOAD FROM S3 DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE LIST CREATION START....\n",
      "Setting up S3 connection for  udacity-dend\n",
      "2021-01-18 11:14:16.531331\n",
      "Searching song data\n",
      "Finished checking, found  5  files\n",
      "Searching log events data\n",
      "Finished checking, found  5  files\n",
      "2021-01-18 11:14:17.949877\n",
      "{\"entries\": \"[{\"url\":\"s3://udacity-dend/song_data/log_data/2018/11/2018-11-01-events.json\"},{\"url\":\"s3://udacity-dend/song_data/log_data/2018/11/2018-11-02-events.json\"},{\"url\":\"s3://udacity-dend/song_data/log_data/2018/11/2018-11-03-events.json\"},{\"url\":\"s3://udacity-dend/song_data/log_data/2018/11/2018-11-04-events.json\"},{\"url\":\"s3://udacity-dend/song_data/log_data/2018/11/2018-11-05-events.json\"}]\"}\n",
      "File  events.manifest  created successfully\n",
      "{\"entries\": \"[{\"url\":\"s3://udacity-dend/song_data/log_data/2018/11/2018-11-01-events.json\"},{\"url\":\"s3://udacity-dend/song_data/log_data/2018/11/2018-11-02-events.json\"},{\"url\":\"s3://udacity-dend/song_data/log_data/2018/11/2018-11-03-events.json\"},{\"url\":\"s3://udacity-dend/song_data/log_data/2018/11/2018-11-04-events.json\"},{\"url\":\"s3://udacity-dend/song_data/log_data/2018/11/2018-11-05-events.json\"}]\"}\n",
      "File  songs.manifest  created successfully\n",
      "SCRIPT DONE\n"
     ]
    }
   ],
   "source": [
    "# S3-File List Creator (creates a manifest JSON file)\n",
    "import boto3\n",
    "import configparser\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "### Check S3 storage for a partial set of the data, download JSON files and load them into DB\n",
    "print(\"FILE LIST CREATION START....\")\n",
    "\n",
    "# Some configurations and initialisations\n",
    "numfil = 10 # number of sample files to download\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dwh.cfg')\n",
    "KEY=config.get('AWS','KEY')\n",
    "SECRET= config.get('AWS','SECRET')\n",
    "BUCKET = config.get('S3', 'S3_BUCKET')\n",
    "S3_BCKT_ROOT = config.get('S3', 'S3_BCKT_ROOT')\n",
    "songdatafiles = list(())\n",
    "eventfiles = list(())\n",
    "\n",
    "# Connect to S3\n",
    "print(\"Setting up S3 connection for \", BUCKET)\n",
    "s3 = boto3.resource('s3',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                     )\n",
    "projectBucket =  s3.Bucket(BUCKET)\n",
    "\n",
    "# Check S3 for song data using \"song-data\" as prefix\n",
    "print(datetime.now())\n",
    "print(\"Searching song data\")\n",
    "for songfile in projectBucket.objects.filter(Prefix=\"song-data\").limit(6):\n",
    "    if \"json\" in songfile.key:\n",
    "        songdatafiles.append(S3_BCKT_ROOT + '/song_data/' + songfile.key)\n",
    "print(\"Finished checking, found \", len(songdatafiles), \" files\")\n",
    "\n",
    "# Check S3 for events using \"log_data\" as prefi\n",
    "print(\"Searching log events data\")\n",
    "for eventfile in projectBucket.objects.filter(Prefix=\"log_data\").limit(6):\n",
    "    if \"json\" in eventfile.key:\n",
    "        eventfiles.append(S3_BCKT_ROOT + '/song_data/' + eventfile.key)\n",
    "print(\"Finished checking, found \", len(eventfiles), \" files\")\n",
    "print(datetime.now())\n",
    "\n",
    "import json\n",
    "events = pd.DataFrame(eventfiles, columns=['url'])\n",
    "events_str = events.to_json(orient='records')\n",
    "events_str = json.dumps({ 'entries': events_str})\n",
    "events_str = events_str.replace('\\\\', '')\n",
    "print(events_str)\n",
    "\n",
    "with open('events.manifest', 'w') as f:\n",
    "    f.write(events_str)\n",
    "    f.close()\n",
    "print('File ', f.name, ' created successfully')\n",
    "\n",
    "songs = pd.DataFrame(eventfiles, columns=['url'])\n",
    "songs_str = songs.to_json(orient='records')\n",
    "songs_str = json.dumps({ 'entries': songs_str})\n",
    "songs_str = songs_str.replace('\\\\', '')\n",
    "print(songs_str)\n",
    "\n",
    "with open('songs.manifest', 'w') as f:\n",
    "    f.write(songs_str)\n",
    "    f.close()\n",
    "\n",
    "print('File ', f.name, ' created successfully')\n",
    "print(\"SCRIPT DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "an integer is required (got type str)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-820a985f3179>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mDWH_PORT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'439'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAF_INET\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDWH_ENDPOINT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDWH_PORT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpeername\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: an integer is required (got type str)"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "DWH_ENDPOINT = 'dwhclustersts.cdzpwfcnkher.us-west-2.redshift.amazonaws.com'\n",
    "DWH_PORT = '439'\n",
    "with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "    s.connect((DWH_ENDPOINT, DWH_PORT))\n",
    "    print(s.getpeername())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Load into staging tables\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import configparser\n",
    "import os\n",
    "from sql_queries import staging_events_insert, staging_songs_insert\n",
    "\n",
    "# Some configurations and initialisations\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dwh.cfg')\n",
    "# Search \"tmp/\" for JSON files to upload into DB\n",
    "filelist = os.listdir('tmp/')\n",
    "print(\"LOAD DATA TO DB STAGING TABLES\")\n",
    "print(\"Found \", len(filelist), \" files\")\n",
    "print(filelist)\n",
    "\n",
    "# Set up connection\n",
    "print(\"Connecting to database\")\n",
    "conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(*config['CLUSTER'].values()))\n",
    "conn.set_session(autocommit=True)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Load data into DB\n",
    "for filename in filelist:\n",
    "    location = \"tmp/\" + filename\n",
    "    print(\"Load \", location)\n",
    "    if \"events\" in filename:\n",
    "        df = pd.read_json(location, lines=True, dtype=True)\n",
    "        events_table_columns = df[[\"artist\", \"auth\", \"firstName\", \"gender\", \"itemInSession\", \"lastName\", \"length\", \"level\", \"location\", \"method\", \"page\", \"registration\", \"sessionId\", \"song\", \"status\", \"ts\", \"userAgent\", \"userId\"]]\n",
    "        events_data = events_table_columns.values[0].tolist()\n",
    "        try:\n",
    "            cur.execute(staging_events_insert, events_data)\n",
    "            print(\"Success\")\n",
    "        except psycopg2.Error as e:\n",
    "            print(\"Insert failed: \", e)\n",
    "    else:\n",
    "        if not filename.startswith(\".\"):\n",
    "            df = pd.read_json(location, lines=True, dtype=True)\n",
    "            songs_table_columns = df[[\"num_songs\",\"artist_id\",\"artist_latitude\",\"artist_longitude\",\"artist_location\", \"artist_name\", \"song_id\", \"title\", \"duration\", \"year\"]]\n",
    "            songs_data = songs_table_columns.values[0].tolist()\n",
    "            try:\n",
    "                cur.execute(staging_songs_insert, songs_data)\n",
    "                print(\"Success\")\n",
    "            except psycopg2.Error as e:\n",
    "                print(\"Insert failed: \", e)\n",
    " \n",
    "conn.close()\n",
    "print(\"LOADING DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 Setting up connections\n",
      "SUCCESS creating REDSHIFT client  <class 'botocore.client.Redshift'>\n",
      "Trying db connection...\n",
      "DB connected...\n",
      "Table  staging_songs  has  14896  entries.\n",
      "Table  staging_events  has  8056  entries.\n",
      "Table  users  has  105  entries.\n",
      "Table  songs  has  29792  entries.\n",
      "Table  time  has  8056  entries.\n",
      "Table  songplays  has  3432  entries.\n"
     ]
    },
    {
     "ename": "IllegalCharacterError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalCharacterError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6c1910e277e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'results.xlsx'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mstaging_events\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheet_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Staging_events'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0mstaging_songs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheet_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Staging_songs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mto_excel\u001b[0;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, encoding, inf_rep, verbose, freeze_panes)\u001b[0m\n\u001b[1;32m   1764\u001b[0m         formatter.write(excel_writer, sheet_name=sheet_name, startrow=startrow,\n\u001b[1;32m   1765\u001b[0m                         \u001b[0mstartcol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstartcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreeze_panes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfreeze_panes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1766\u001b[0;31m                         engine=engine)\n\u001b[0m\u001b[1;32m   1767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1768\u001b[0m     def to_stata(self, fname, convert_dates=None, write_index=True,\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/formats/excel.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine)\u001b[0m\n\u001b[1;32m    650\u001b[0m         writer.write_cells(formatted_cells, sheet_name,\n\u001b[1;32m    651\u001b[0m                            \u001b[0mstartrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstartrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstartcol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstartcol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m                            freeze_panes=freeze_panes)\n\u001b[0m\u001b[1;32m    653\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mneed_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/excel.py\u001b[0m in \u001b[0;36mwrite_cells\u001b[0;34m(self, cells, sheet_name, startrow, startcol, freeze_panes)\u001b[0m\n\u001b[1;32m   1393\u001b[0m                 \u001b[0mcolumn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstartcol\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m             )\n\u001b[0;32m-> 1395\u001b[0;31m             \u001b[0mxcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_with_fmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1396\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m                 \u001b[0mxcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/openpyxl/cell/cell.py\u001b[0m in \u001b[0;36mvalue\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;34m\"\"\"Set the value and infer type and display options.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bind_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/openpyxl/cell/cell.py\u001b[0m in \u001b[0;36m_bind_value\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTRING_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTYPE_STRING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/openpyxl/cell/cell.py\u001b[0m in \u001b[0;36mcheck_string\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m32767\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mILLEGAL_CHARACTERS_RE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIllegalCharacterError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalCharacterError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Check if tables contain data\n",
    "import configparser\n",
    "import psycopg2\n",
    "from sql_queries import create_table_queries, drop_table_queries\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def create_redshift_client(KEY, SECRET):\n",
    "    try:\n",
    "        print(\"1.1 Setting up connections\") \n",
    "        redshift = boto3.client('redshift',\n",
    "                               region_name=\"us-west-2\",\n",
    "                               aws_access_key_id=KEY,\n",
    "                               aws_secret_access_key=SECRET\n",
    "                               )\n",
    "        print(\"SUCCESS creating REDSHIFT client \", type(redshift))\n",
    "    except Exception as e:\n",
    "        print(\"FAILED creating REDSHIFT client: \", e)\n",
    "    return redshift\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dwh.cfg')\n",
    "KEY                    = config.get('AWS','KEY')\n",
    "SECRET                 = config.get('AWS','SECRET')\n",
    "DWH_DB                 = config.get(\"DWH\",\"DWH_DB\")\n",
    "DWH_DB_USER            = config.get(\"DWH\",\"DWH_DB_USER\")\n",
    "DWH_DB_PASSWORD        = config.get(\"DWH\",\"DWH_DB_PASSWORD\")\n",
    "DWH_PORT               = config.get(\"DWH\",\"DWH_PORT\")\n",
    "redshift_client = create_redshift_client(KEY, SECRET)\n",
    "DWH_CLUSTER_IDENTIFIER = config.get(\"DWH\",\"DWH_CLUSTER_IDENTIFIER\")\n",
    "myClusterProps = redshift_client.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "DWH_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "params = [DWH_ENDPOINT, DWH_DB, DWH_DB_USER, DWH_DB_PASSWORD, DWH_PORT]\n",
    "\n",
    "print(\"Trying db connection...\")\n",
    "try:\n",
    "    conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(*params))\n",
    "    conn.set_session(autocommit=True)\n",
    "    cur = conn.cursor()\n",
    "    print(\"DB connected...\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "select = \"SELECT * FROM \"\n",
    "tables = [\"staging_songs\", \"staging_events\", \"users\", \"songs\", \"time\", \"songplays\"]\n",
    "delim = \";\"\n",
    "\n",
    "for tab in tables:\n",
    "    query = select + tab + delim\n",
    "    cur.execute(query)\n",
    "    response = cur.fetchall()\n",
    "    print(\"Table \", tab, \" has \", len(response), \" entries.\")\n",
    "    \n",
    "    \n",
    "song_stage_columns=['st_song_id', 'num_songs', 'artist_id', 'artist_latitude', 'artist_longitude', \n",
    "                        'artist_location', 'artist_name', 'song_id', 'title', 'length', 'year']\n",
    "event_stage_columns=['artist', 'auth', 'firstname', 'gender', 'iteminsession', \n",
    "                         'lastname', 'length', 'level', 'location', 'method', 'page', 'registration', \n",
    "                         'sessionid', 'song', 'status', 'ts', 'useragent', 'userid']\n",
    "cur.execute('SELECT * from staging_events') \n",
    "staging_events =  pd.DataFrame(cur.fetchall(), columns=event_stage_columns)\n",
    "staging_songs = pd.DataFrame(cur.fetchall(), columns=song_stage_columns)\n",
    "\n",
    "with open('results.xlsx', 'w') as f:\n",
    "    staging_events.to_excel(f, sheet_name='Staging_events')\n",
    "    staging_songs.to_excel(f, sheet_name='Staging_songs')\n",
    "\n",
    "    \n",
    "print(datetime.now(), ': Selected ', len(staging_events), ' rows from staging_events')\n",
    "staging_events = staging_events.drop_duplicates()\n",
    "cur.execute('SELECT * from staging_songs') \n",
    "staging_songs = pd.DataFrame(cur.fetchall(), columns=song_stage_columns)\n",
    "print(datetime.now(), ': Selected ', len(staging_songs), ' rows from staging_songs')\n",
    "staging_songs = staging_songs.drop_duplicates()\n",
    "song_columns=['song_id','title','artist_id','year','length']\n",
    "songplay_columns=['ts', 'userid', 'level', 'song_id', 'artist_id', 'sessionid', 'location', 'useragent']\n",
    "artist_columns=['artist_id', 'artist_name', 'artist_location', 'artist_latitude', 'artist_longitude']\n",
    "\n",
    "\n",
    "# Filter events table for songplay events and remove missing data for key columns\n",
    "events_df = staging_events.query('page == \"NextSong\"').copy()\n",
    "events_df.dropna(subset=['userid', 'ts', 'song'], how='any')\n",
    "\n",
    "artist_df = staging_songs[artist_columns].copy()\n",
    "artist_df.dropna(subset=['artist_name'])\n",
    "artist_df.drop_duplicates(subset='artist_id')\n",
    "#print(artist_df.head())\n",
    "\n",
    "songs_df = staging_songs[song_columns].copy()\n",
    "songs_df.drop_duplicates('song_id')\n",
    "sp_events = events_df[['ts', 'userid', 'level', 'sessionid', 'location', 'useragent', 'song']]\n",
    "sp_songs = songs_df[['song_id', 'artist_id', 'title']]\n",
    "sp_df = sp_events.merge(sp_songs, left_on='song', right_on='title')\n",
    "sp_df = sp_df[songplay_columns]\n",
    "print(songs_df.loc[songs_df['title'] == 'Becoming Insane'].head(), events_df.loc[events_df['song'] == 'Becoming Insane'].head())\n",
    "#print(sp_df.head())\n",
    "    \n",
    "conn.close()\n",
    "\n",
    "print(\"Connection closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "load = %load_ext sql\n",
    "conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DWH_DB_USER, DWH_DB_PASSWORD, DWH_ENDPOINT, DWH_PORT,DWH_DB)\n",
    "connected = %sql $conn_string\n",
    "\n",
    "#%sql insert into songplays (start_time, user_id, level, song_id, artist_id, session_id, location, user_agent) select e.ts, e.userid, e.level, s.song_id, s.artist_id, e.sessionid, e.location, e.useragent from songs as s left outer join staging_events as e on e.song like s.title where e.page like 'NextSong'\n",
    "#%sql select e.ts, e.userid, e.level, s.song_id, s.artist_id, e.sessionid, e.location, e.useragent from songs as s left outer join staging_events as e on e.song like s.title where e.page like 'NextSong'\n",
    "\n",
    "%sql select * from songplays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-91-69571cc4365a>, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-91-69571cc4365a>\"\u001b[0;36m, line \u001b[0;32m23\u001b[0m\n\u001b[0;31m    def process_event_file(cur, filepath)\u001b[0m\n\u001b[0m                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from sql_queries import *\n",
    "\n",
    "\n",
    "def process_song_file(cur, filepath):\n",
    "    \"\"\"Takes the json file provided in filepath and reads the file\n",
    "    Then song data and artist data are selected and inserted into the corresponding tables\"\"\"\n",
    "    # Open song file using the path variable \"filepath\" and create a dataframe from it\n",
    "    df = pd.read_json(filepath, lines=True)\n",
    "    \n",
    "    # Insert songs record\n",
    "    # Select song data columns from df and store in a new dataframe\n",
    "    songs_table_columns = df[[\"num_songs\",\"artist_id\",\"artist_latitude\",\"artist_longitude\",\"artist_location\", \"artist_name\", \"song_id\", \"title\", \"duration\", \"year\"]]\n",
    "\n",
    "    # Select only the values from df and store in a list\n",
    "    song_data = songs_table_columns.values[0].tolist()\n",
    "    # Write to songs table\n",
    "    cur.execute(song_table_insert, song_data)\n",
    "    \n",
    "def process_event_file(cur, filepath)\n",
    "    df = pd.read_json(filepath, lines=True)\n",
    "    events_table_columns = df[[\"artist\", \"auth\", \"firstName\", \"gender\", \"itemInSession\", \"lastName\", \"length\", \"level\", \"location\", \"method\", \"page\", \"registration\", \"sessionId\", \"song\", \"status\", \"ts\", \"userAgent\", \"userId\"]]\n",
    "    events_data = events_table_columns.values[0].tolist()\n",
    "\n",
    "    \n",
    "def process_data(cur, conn, filepath, func):\n",
    "    \"\"\"Recursively search given filepath and process all json files.\n",
    "    Calls for each file the function \"func\" (which is either directing from here to \"process_song_file\" or \"process_log_file\n",
    "    Nothing returned from here, but the changes from the sub-functions are committed\"\"\"\n",
    "    # et all files matching extension from directory\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(filepath):\n",
    "        files = glob.glob(os.path.join(root,'*.json'))\n",
    "        for f in files :\n",
    "            all_files.append(os.path.abspath(f))\n",
    "\n",
    "    # get total number of files found\n",
    "    num_files = len(all_files)\n",
    "    print('{} files found in {}'.format(num_files, filepath))\n",
    "\n",
    "    # iterate over files and process\n",
    "    for i, datafile in enumerate(all_files, 1):\n",
    "        func(cur, datafile)\n",
    "        conn.commit()\n",
    "        print('{}/{} files processed.'.format(i, num_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 Reading config file\n",
      "Done\n",
      "1.2 Setting up connections\n",
      "SUCCESS creating IAM client  <class 'botocore.client.IAM'>\n",
      "1.3 Creating a new IAM Role\n",
      "1.4 Attaching Policy\n",
      "1.5 Get the IAM role ARN\n",
      "arn:aws:iam::422675603730:role/dwhRole\n"
     ]
    }
   ],
   "source": [
    "### CREATE IAM ROLE AND POLICY\n",
    "from botocore.exceptions import ClientError\n",
    "import boto3\n",
    "import configparser\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Read config items from file and setup connections\n",
    "print(\"1.1 Reading config file\") \n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))\n",
    "KEY                    = config.get('AWS','KEY')\n",
    "SECRET                 = config.get('AWS','SECRET')\n",
    "DWH_CLUSTER_TYPE       = config.get(\"DWH\",\"DWH_CLUSTER_TYPE\")\n",
    "DWH_NUM_NODES          = config.get(\"DWH\",\"DWH_NUM_NODES\")\n",
    "DWH_NODE_TYPE          = config.get(\"DWH\",\"DWH_NODE_TYPE\")\n",
    "DWH_CLUSTER_IDENTIFIER = config.get(\"DWH\",\"DWH_CLUSTER_IDENTIFIER\")\n",
    "DWH_DB                 = config.get(\"DWH\",\"DWH_DB\")\n",
    "DWH_DB_USER            = config.get(\"DWH\",\"DWH_DB_USER\")\n",
    "DWH_DB_PASSWORD        = config.get(\"DWH\",\"DWH_DB_PASSWORD\")\n",
    "DWH_PORT               = config.get(\"DWH\",\"DWH_PORT\")\n",
    "DWH_IAM_ROLE_NAME      = config.get(\"DWH\", \"DWH_IAM_ROLE_NAME\")\n",
    "pd.DataFrame({\"Param\":\n",
    "                  [\"DWH_CLUSTER_TYPE\", \"DWH_NUM_NODES\", \"DWH_NODE_TYPE\", \"DWH_CLUSTER_IDENTIFIER\", \"DWH_DB\", \"DWH_DB_USER\", \"DWH_DB_PASSWORD\", \"DWH_PORT\", \"DWH_IAM_ROLE_NAME\"],\n",
    "              \"Value\":\n",
    "                  [DWH_CLUSTER_TYPE, DWH_NUM_NODES, DWH_NODE_TYPE, DWH_CLUSTER_IDENTIFIER, DWH_DB, DWH_DB_USER, DWH_DB_PASSWORD, DWH_PORT, DWH_IAM_ROLE_NAME]\n",
    "             })\n",
    "print(\"Done\")\n",
    "\n",
    "try:\n",
    "    print(\"1.2 Setting up connections\") \n",
    "    iam = boto3.client('iam',\n",
    "                           region_name=\"us-west-2\",\n",
    "                           aws_access_key_id=KEY,\n",
    "                           aws_secret_access_key=SECRET\n",
    "                           )\n",
    "    print(\"SUCCESS creating IAM client \", type(iam))\n",
    "except Exception as e:\n",
    "    print(\"FAILED creating IAM client: \", e)\n",
    "\n",
    "try:\n",
    "    print(\"1.3 Creating a new IAM Role\") \n",
    "    dwhRole = iam.create_role(\n",
    "        Path='/',\n",
    "        RoleName=DWH_IAM_ROLE_NAME,\n",
    "        Description = \"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "        AssumeRolePolicyDocument=json.dumps(\n",
    "            {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "               'Effect': 'Allow',\n",
    "               'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "             'Version': '2012-10-17'})\n",
    "    )    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "print(\"1.4 Attaching Policy\")\n",
    "iam.attach_role_policy(RoleName=DWH_IAM_ROLE_NAME,\n",
    "                       PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                      )['ResponseMetadata']['HTTPStatusCode']\n",
    "print(\"1.5 Get the IAM role ARN\")\n",
    "roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn']\n",
    "print(roleArn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATE REDSHIFT CLUSTER\n",
    "from botocore.exceptions import ClientError\n",
    "import boto3\n",
    "import configparser\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Read config items from file and setup connections\n",
    "print(\"1.1 Reading config file\") \n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))\n",
    "KEY                    = config.get('AWS','KEY')\n",
    "SECRET                 = config.get('AWS','SECRET')\n",
    "DWH_CLUSTER_TYPE       = config.get(\"DWH\",\"DWH_CLUSTER_TYPE\")\n",
    "DWH_NUM_NODES          = config.get(\"DWH\",\"DWH_NUM_NODES\")\n",
    "DWH_NODE_TYPE          = config.get(\"DWH\",\"DWH_NODE_TYPE\")\n",
    "DWH_CLUSTER_IDENTIFIER = config.get(\"DWH\",\"DWH_CLUSTER_IDENTIFIER\")\n",
    "DWH_DB                 = config.get(\"DWH\",\"DWH_DB\")\n",
    "DWH_DB_USER            = config.get(\"DWH\",\"DWH_DB_USER\")\n",
    "DWH_DB_PASSWORD        = config.get(\"DWH\",\"DWH_DB_PASSWORD\")\n",
    "DWH_PORT               = config.get(\"DWH\",\"DWH_PORT\")\n",
    "DWH_IAM_ROLE_NAME      = config.get(\"DWH\", \"DWH_IAM_ROLE_NAME\")\n",
    "pd.DataFrame({\"Param\":\n",
    "                  [\"DWH_CLUSTER_TYPE\", \"DWH_NUM_NODES\", \"DWH_NODE_TYPE\", \"DWH_CLUSTER_IDENTIFIER\", \"DWH_DB\", \"DWH_DB_USER\", \"DWH_DB_PASSWORD\", \"DWH_PORT\", \"DWH_IAM_ROLE_NAME\"],\n",
    "              \"Value\":\n",
    "                  [DWH_CLUSTER_TYPE, DWH_NUM_NODES, DWH_NODE_TYPE, DWH_CLUSTER_IDENTIFIER, DWH_DB, DWH_DB_USER, DWH_DB_PASSWORD, DWH_PORT, DWH_IAM_ROLE_NAME]\n",
    "             })\n",
    "print(\"Done\")\n",
    "\n",
    "try:\n",
    "    print(\"1.1 Setting up connections\") \n",
    "    redshift = boto3.client('redshift',\n",
    "                           region_name=\"us-west-2\",\n",
    "                           aws_access_key_id=KEY,\n",
    "                           aws_secret_access_key=SECRET\n",
    "                           )\n",
    "    print(\"SUCCESS creating REDSHIFT client \", type(redshift))\n",
    "except Exception as e:\n",
    "    print(\"FAILED creating REDSHIFT client: \", e)\n",
    "contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    print(\"1.2 Create cluster\") \n",
    "    response = redshift.create_cluster(        \n",
    "        #HW\n",
    "        ClusterType=DWH_CLUSTER_TYPE,\n",
    "        NodeType=DWH_NODE_TYPE,\n",
    "        #NumberOfNodes=int(DWH_NUM_NODES),\n",
    "\n",
    "        #Identifiers & Credentials\n",
    "        DBName=DWH_DB,\n",
    "        ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
    "        MasterUsername=DWH_DB_USER,\n",
    "        MasterUserPassword=DWH_DB_PASSWORD,\n",
    "        \n",
    "        #Roles (for s3 access)\n",
    "        c\n",
    "    )\n",
    "    print(\"SUCCESS creating cluster: \", str(response.get(\"ClusterCreateTime\")))\n",
    "except Exception as e:\n",
    "    print(\"FAILED creating cluster: \", e)\n",
    "\n",
    "def prettyRedshiftProps(props):\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
    "\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENABLE TCP CONNECTION\n",
    "from botocore.exceptions import ClientError\n",
    "import boto3\n",
    "import configparser\n",
    "import pandas as pd\n",
    "\n",
    "# Wait for Redshift instance to come up\n",
    "redshift_waiter = redshift.get_waiter('cluster_available')\n",
    "redshift_waiter.wait(ClusterIdentstr(sys.argv)ifier='dwhClusterSTS')\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "\n",
    "try:\n",
    "    print(\"1.1 Setting up connections\") \n",
    "    ec2 = boto3.resource('ec2',\n",
    "                           region_name=\"us-west-2\",\n",
    "                           aws_access_key_id=KEY,\n",
    "                           aws_secret_access_key=SECRET\n",
    "                           )\n",
    "    print(\"SUCCESS creating EC2 client \", type(iam))\n",
    "except Exception as e:\n",
    "    print(\"FAILED creating EC2 client: \", e)\n",
    "\n",
    "print(\"1.2 Open TCP Port\")\n",
    "DWH_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']\n",
    "print(\"DWH_ENDPOINT :: \", DWH_ENDPOINT)\n",
    "print(\"DWH_ROLE_ARN :: \", DWH_ROLE_ARN)\n",
    "try:\n",
    "    vpc = ec2.Vpc(id=myClusterProps['VpcId'])\n",
    "    defaultSg = list(vpc.security_groups.all())[0]\n",
    "    print(defaultSg)\n",
    "    defaultSg.authorize_ingress(\n",
    "        GroupName=defaultSg.group_name,\n",
    "        CidrIp='0.0.0.0/0',\n",
    "        IpProtocol='TCP',\n",
    "        FromPort=int(DWH_PORT),\n",
    "        ToPort=int(DWH_PORT)\n",
    "    )\n",
    "    print(\"SUCCESS opening TCP Port\")\n",
    "except Exception as e:\n",
    "    print(\"FAILED opening TCP Port \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhclustersts.cdzpwfcnkher.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "2 rows affected.\n",
      " * postgresql://dwhuser:***@dwhclustersts.cdzpwfcnkher.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "0 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>query</th>\n",
       "        <th>substring</th>\n",
       "        <th>line</th>\n",
       "        <th>value</th>\n",
       "        <th>err_reason</th>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext sql\n",
    "conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DWH_DB_USER, DWH_DB_PASSWORD, DWH_ENDPOINT, DWH_PORT,DWH_DB)\n",
    "%sql $conn_string\n",
    "# List Load Errors\n",
    "%sql select filename, line_number, colname, type, position, raw_line, err_code, err_reason from stl_load_errors;\n",
    "# Show Load Error details\n",
    "%sql select d.query, substring(d.filename,14,20), d.line_number as line, substring(d.value,1,16) as value, substring(le.err_reason,1,48) as err_reason from stl_loaderror_detail d, stl_load_errors le where d.query = le.query and d.query = pg_last_copy_id();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA LOAD\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import configparser\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sql_queries import staging_events_insert, staging_songs_insert\n",
    "from create_tables import create_client\n",
    "\n",
    "# Some configurations and initialisations\n",
    "now = datetime.now()\n",
    "KEY                    = config.get('AWS','KEY')\n",
    "SECRET                 = config.get('AWS','SECRET')\n",
    "DWH_DB                 = config.get(\"DWH\",\"DWH_DB\")\n",
    "DWH_DB_USER            = config.get(\"DWH\",\"DWH_DB_USER\")\n",
    "DWH_DB_PASSWORD        = config.get(\"DWH\",\"DWH_DB_PASSWORD\")\n",
    "DWH_PORT               = config.get(\"DWH\",\"DWH_PORT\")\n",
    "redshift = create_client(KEY, SECRET, 'redshift')\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dwh.cfg')\n",
    "S3_BCKT_ROOT                    = config.get('S3','S3_BCKT_ROOT') + \"/\"\n",
    "root = pd.Series(S3_BCKT_ROOT)\n",
    "S3_ROOT                    = config.get('S3','S3_BUCKET')\n",
    "LOG_JSONPATH                    = config.get('S3','LOG_JSONPATH')\n",
    "SONG_DATA                    = config.get('S3','SONG_DATA')\n",
    "LOG_DATA                    = config.get('S3','LOG_DATA')\n",
    "\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "DWH_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']\n",
    "region_name=\"us-west-2\"\n",
    "\n",
    "# Set up connection\n",
    "print(now, \": Connecting to database\")\n",
    "conn = psycopg2.connect(host=DWH_ENDPOINT, dbname=DWH_DB, user=DWH_DB_USER, password=DWH_DB_PASSWORD, port=DWH_PORT)\n",
    "conn.set_session(autocommit=True)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Search \"tmp/\" for JSON files to upload into DB\n",
    "filelist = os.listdir('tmp/')\n",
    "songsample = list()\n",
    "eventsample = list()\n",
    "print(\"LOAD DATA TO DB STAGING TABLES\")\n",
    "for file in filelist:\n",
    "    if \"json\" in file:\n",
    "        if \"events\" in file:\n",
    "            eventsample.append(file)\n",
    "        else:\n",
    "            songsample.append(file)\n",
    "print(\"Found \", (len(eventsample) + len(songsample)), \" files\")\n",
    "\n",
    "s3songs = pd.Series(songdatafiles)\n",
    "s3events = pd.Series(eventfiles)\n",
    "s3songsample = pd.Series(s3songs[0:10])\n",
    "s3songsample = root.values + s3songsample.values\n",
    "s3eventsample = pd.Series(s3events[0:10])\n",
    "s3eventsample = root.values + s3eventsample.values\n",
    "\n",
    "for file in s3eventsample:\n",
    "    copy_events = \"copy staging_events from %s iam_role %s compupdate off blanksasnull emptyasnull region %s format as json \\'auto ignorecase\\'\"\n",
    "    try:\n",
    "        cur.execute(copy_events, [file, DWH_ROLE_ARN, region_name])\n",
    "        print(cur.fetchall())\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    print(\"Loading \", file, \" done\")\n",
    "    \n",
    "for file in s3songsample:\n",
    "    copy_events = \"copy staging_songs from %s iam_role %s compupdate off blanksasnull emptyasnull region %s format as json \\'auto ignorecase\\'\"\n",
    "    try:\n",
    "        cur.execute(copy_events, [file, DWH_ROLE_ARN, region_name])\n",
    "        print(cur.fetchall())\n",
    "        print(\"Loading \", file, \" done\")\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redshift.delete_cluster( ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,  SkipFinalClusterSnapshot=True)\n",
    "#iam.detach_role_policy(RoleName=DWH_IAM_ROLE_NAME, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")\n",
    "#iam.delete_role(RoleName=DWH_IAM_ROLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
