{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- num_songs: long (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n",
      "None None\n"
     ]
    }
   ],
   "source": [
    "### STEP 1: Read Input Files from S3 (or local if applicable\n",
    "# (inferred schema version)\n",
    "staging_songs = spark.read.json('data/song_data/*/*/*/*.json', encoding='UTF-8')\n",
    "staging_events = spark.read.json('data/2018*.json')\n",
    "\n",
    "staging_songs.createOrReplaceTempView(\"staging_songs\")\n",
    "staging_events.createOrReplaceTempView(\"staging_events\")\n",
    "\n",
    "print(staging_songs.printSchema(), staging_events.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "analytics_tables = ['songs', 'users', 'time', 'artists', 'songplays'] \n",
    "staging_tables = ['staging_events', 'staging_songs']\n",
    "\n",
    "song_columns=['song_id','title','artist_id','year','length']\n",
    "user_columns=['userid', 'firstname', 'lastname', 'gender', 'level']\n",
    "time_columns =['timestamp', 'hour', 'day', 'week', 'month', 'year', 'weekday']\n",
    "artist_columns=['artist_id', 'artist_name', 'artist_location', 'artist_latitude', 'artist_longitude']\n",
    "songplay_export_colums=['songplay_id','start_time', 'userid', 'level', 'song_id', 'artist_id', 'sessionid', 'location', 'useragent', 'year', 'month']\n",
    "songplay_columns=['songplay_id','start_time', 'userid', 'level', 'song_id', 'artist_id', 'sessionid', 'location', 'useragent']\n",
    "\n",
    "artist_table_insert = (\"\"\"\n",
    "    select artist_id, artist_name, artist_location, artist_latitude, artist_longitude\n",
    "    from staging_songs\n",
    "    where artist_name is not null\n",
    "    \"\"\")\n",
    "\n",
    "time_table_insert = (\"\"\"\n",
    "    select ts,\n",
    "    extract (hour from start_time) as hour,\n",
    "    extract (day from start_time) as day,\n",
    "    extract (week from start_time) as week,\n",
    "    extract (month from start_time) as month,\n",
    "    extract (year from start_time) as year,\n",
    "    extract (dayofweek from start_time) as weekday\n",
    "    from staging_events\n",
    "    where ts is not null\n",
    "    \"\"\")\n",
    "\n",
    "song_table_insert = (\"\"\"\n",
    "    select song_id, title, artist_id, year, duration\n",
    "    from staging_songs\n",
    "    where title is not null\n",
    "    \"\"\")\n",
    "\n",
    "user_table_insert = (\"\"\"\n",
    "    select userid, firstname, lastname, gender, level\n",
    "    from staging_events\n",
    "    where (firstname, lastname) is not null\n",
    "    \"\"\")\n",
    "\n",
    "songplay_table_insert_old = (\"\"\"\n",
    "    select e.ts, e.userid, e.level, s.song_id, s.artist_id, e.sessionid, e.location, e.useragent\n",
    "    from songs as s\n",
    "    left outer join staging_events as e\n",
    "    on e.song like s.title\n",
    "    where e.page like 'NextSong'\n",
    "    \"\"\")\n",
    "\n",
    "songplay_table_insert = (\"\"\"\n",
    "    select e.ts, e.userid, e.level, s.song_id, s.artist_id, e.sessionid, e.location, e.useragent\n",
    "    from staging_events as e\n",
    "    join staging_songs as s\n",
    "    on e.song like s.title\n",
    "    where e.page like 'NextSong' and e.song is not null and s.artist_id is not null\n",
    "    and e.artist like s.artist_name\n",
    "    \"\"\")\n",
    "\n",
    "insert_table_queries = [user_table_insert, song_table_insert, artist_table_insert, time_table_insert, songplay_table_insert]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- song_id: string (nullable = false)\n",
      " |-- title: string (nullable = true)\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- duration: float (nullable = true)\n",
      "\n",
      "None Number of entries:  71\n",
      "root\n",
      " |-- artist_id: string (nullable = false)\n",
      " |-- artist_name: string (nullable = false)\n",
      " |-- location: string (nullable = true)\n",
      " |-- latitude: float (nullable = true)\n",
      " |-- longitude: float (nullable = true)\n",
      "\n",
      "None Number of entries:  69\n",
      "root\n",
      " |-- userid: integer (nullable = false)\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      "\n",
      "None Number of entries:  97\n",
      "root\n",
      " |-- start_time: timestamp (nullable = false)\n",
      " |-- hour: integer (nullable = false)\n",
      " |-- day: integer (nullable = false)\n",
      " |-- week: integer (nullable = false)\n",
      " |-- month: integer (nullable = false)\n",
      " |-- year: integer (nullable = false)\n",
      " |-- dayofweek: integer (nullable = false)\n",
      "\n",
      "None Number of entries:  1\n",
      "root\n",
      " |-- songplay_id: integer (nullable = false)\n",
      " |-- start_time: timestamp (nullable = false)\n",
      " |-- userid: integer (nullable = false)\n",
      " |-- level: string (nullable = true)\n",
      " |-- song_id: string (nullable = false)\n",
      " |-- artist_id: string (nullable = false)\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- user_agent: string (nullable = true)\n",
      "\n",
      "None Number of entries:  1\n",
      "+-----------+--------------------+------+-----+------------------+------------------+----------+--------------------+--------------------+\n",
      "|songplay_id|          start_time|userid|level|           song_id|         artist_id|session_id|            location|          user_agent|\n",
      "+-----------+--------------------+------+-----+------------------+------------------+----------+--------------------+--------------------+\n",
      "|          0|2018-11-21 21:56:...|    15| paid|SOZCTXZ12AB0182364|AR5KOSW1187FB35FF4|       818|Chicago-Napervill...|\"Mozilla/5.0 (X11...|\n",
      "+-----------+--------------------+------+-----+------------------+------------------+----------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### STEP 2: Create analytics tables\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, LongType, TimestampType\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "usr_schema = StructType([ \\\n",
    "    StructField(\"userid\",IntegerType(),False), \\\n",
    "    StructField(\"firstname\",StringType(),True), \\\n",
    "    StructField(\"lastname\",StringType(),True), \\\n",
    "    StructField(\"gender\", StringType(), True), \\\n",
    "    StructField(\"level\", StringType(), True), \\\n",
    "    ])\n",
    "\n",
    "song_schema = StructType([ \\\n",
    "    StructField(\"song_id\",StringType(),False), \\\n",
    "    StructField(\"title\",StringType(),True), \\\n",
    "    StructField(\"artist_id\",StringType(),True), \\\n",
    "    StructField(\"year\", IntegerType(), True), \\\n",
    "    StructField(\"duration\", FloatType(), True), \\\n",
    "    ])\n",
    "\n",
    "artist_schema = StructType([ \\\n",
    "    StructField(\"artist_id\",StringType(),False), \\\n",
    "    StructField(\"artist_name\",StringType(),False), \\\n",
    "    StructField(\"location\",StringType(),True), \\\n",
    "    StructField(\"latitude\", FloatType(), True), \\\n",
    "    StructField(\"longitude\", FloatType(), True), \\\n",
    "    ])\n",
    "\n",
    "time_schema = StructType([ \\\n",
    "    StructField(\"start_time\",TimestampType(),False), \\\n",
    "    StructField(\"hour\",IntegerType(),False), \\\n",
    "    StructField(\"day\",IntegerType(),False), \\\n",
    "    StructField(\"week\", IntegerType(), False), \\\n",
    "    StructField(\"month\", IntegerType(), False), \\\n",
    "    StructField(\"year\", IntegerType(), False), \\\n",
    "    StructField(\"dayofweek\", IntegerType(), False), \\\n",
    "    ])\n",
    "\n",
    "songplay_schema = StructType([ \\\n",
    "    StructField(\"songplay_id\",IntegerType(),False), \\\n",
    "    StructField(\"start_time\",TimestampType(),False), \\\n",
    "    StructField(\"userid\", IntegerType(),False), \\\n",
    "    StructField(\"level\", StringType(), True), \\\n",
    "    StructField(\"song_id\", StringType(), False), \\\n",
    "    StructField(\"artist_id\", StringType(), False), \\\n",
    "    StructField(\"session_id\", StringType(), True), \\\n",
    "    StructField(\"location\", StringType(), True), \\\n",
    "    StructField(\"user_agent\", StringType(), True), \\\n",
    "    ])\n",
    "\n",
    "# For each table:\n",
    "#  - check for duplicate ids\n",
    "#  - delete rows for empty key values such as missing user ids\n",
    "#  - rename columns if applicaable\n",
    "#  - calculate inferred values such as hour(timestamp)\n",
    "#  - apply new schema\n",
    "\n",
    "\n",
    "# 2a) songs\n",
    "songs = spark.sql(song_table_insert)\n",
    "songs = songs.dropDuplicates(['song_id'])\n",
    "songs = spark.createDataFrame(songs.collect(),schema=song_schema)\n",
    "songs.createOrReplaceTempView(\"songs\")\n",
    "\n",
    "# 2b) artists\n",
    "artists = spark.sql(artist_table_insert)\n",
    "artists = artists.dropDuplicates(['artist_id'])\n",
    "artists = spark.createDataFrame(artists.collect(),schema=artist_schema)\n",
    "artists.createOrReplaceTempView(\"artists\")\n",
    "\n",
    "\n",
    "# 2c) users\n",
    "users = spark.sql(user_table_insert)\n",
    "users = users.filter(col('userid') != '')\n",
    "users = users.dropDuplicates(['userid'])\n",
    "users = users.withColumn('userid', users.userid.cast(IntegerType()))\n",
    "users = spark.createDataFrame(users.collect(),schema=usr_schema)\n",
    "users.createOrReplaceTempView(\"users\")\n",
    "\n",
    "# 2d) timestamps\n",
    "#time = staging_events.select('ts').filter(staging_events.page == 'NextSong')\n",
    "#time = time.withColumn('ts', to_timestamp(time.ts/1000))\n",
    "#time = time.withColumn('hour', hour(time.ts)).withColumn('hour', hour(time.ts)).withColumn('day', dayofmonth(time.ts)).withColumn('week', weekofyear(time.ts)).withColumn('month', month(time.ts)).withColumn('year', year(time.ts)).withColumn('dayofweek', dayofweek(time.ts))\n",
    "#time.createOrReplaceTempView(\"time\")\n",
    "#content = spark.sql('select distinct * from time').collect()\n",
    "#time = spark.createDataFrame(content,schema=time_schema)\n",
    "#time.createOrReplaceTempView(\"time\")\n",
    "\n",
    "\n",
    "# 2e) songplays\n",
    "songplays = spark.sql(songplay_table_insert)\n",
    "songplays = songplays.withColumn('ts', to_timestamp(songplays.ts/1000))\n",
    "songplays = songplays.withColumnRenamed('ts', 'start_time')\n",
    "songplays = songplays.withColumn('songplay_id', monotonically_increasing_id())\n",
    "songplays = songplays.withColumn('userid', songplays.userid.cast(IntegerType()))\n",
    "songplays = songplays.select(songplay_columns)\n",
    "songplays = spark.createDataFrame(songplays.collect(),schema=songplay_schema)\n",
    "songplays.createOrReplaceTempView(\"songplays\")\n",
    "\n",
    "\n",
    "# 2f) time (new approach, only select timestamps for songplay-entry\n",
    "time = songplays.select('start_time')\n",
    "time = time.withColumn('hour', hour('start_time')).withColumn('hour', hour('start_time')).withColumn('day', dayofmonth('start_time')).withColumn('week', weekofyear('start_time')).withColumn('month', month('start_time')).withColumn('year', year('start_time')).withColumn('dayofweek', dayofweek('start_time'))\n",
    "time = spark.createDataFrame(time.collect(),schema=time_schema)\n",
    "time.createOrReplaceTempView(\"time\")\n",
    "\n",
    "for frame in [songs, artists, users, time, songplays]:\n",
    "    print(frame.printSchema(), 'Number of entries: ', frame.count())\n",
    "\n",
    "songplays.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`ts`' given input columns: [songplays.location, songplays.sessionid, songplays.useragent, songplays.start_time, songplays.level, songplays.song_id, songplays.artist_id, songplays.userid, songplays.year]; line 1 pos 7;\\n'Project ['ts, cast(userid#2915 as int) AS userid#4601, level#2905, song_id#2879, artist_id#2872, sessionid#2910L, location#2906, useragent#2914]\\n+- SubqueryAlias `songplays`\\n   +- Project [start_time#3720, userid#2915, level#2905, song_id#2879, artist_id#2872, sessionid#2910L, location#2906, useragent#2914, year(cast(start_time#3720 as date)) AS year#3729]\\n      +- Project [ts#3711 AS start_time#3720, userid#2915, level#2905, song_id#2879, artist_id#2872, sessionid#2910L, location#2906, useragent#2914]\\n         +- Project [to_timestamp((ts#2913L / 1000), None) AS ts#3711, userid#2915, level#2905, song_id#2879, artist_id#2872, sessionid#2910L, location#2906, useragent#2914]\\n            +- Project [ts#2913L, userid#2915, level#2905, song_id#2879, artist_id#2872, sessionid#2910L, location#2906, useragent#2914]\\n               +- Filter ((page#2908 LIKE NextSong && isnotnull(song#2911)) && (isnotnull(artist_id#2872) && artist#2898 LIKE artist_name#2876))\\n                  +- Join Inner, song#2911 LIKE title#2880\\n                     :- SubqueryAlias `e`\\n                     :  +- SubqueryAlias `staging_events`\\n                     :     +- Relation[artist#2898,auth#2899,firstName#2900,gender#2901,itemInSession#2902L,lastName#2903,length#2904,level#2905,location#2906,method#2907,page#2908,registration#2909,sessionId#2910L,song#2911,status#2912L,ts#2913L,userAgent#2914,userId#2915] json\\n                     +- SubqueryAlias `s`\\n                        +- SubqueryAlias `staging_songs`\\n                           +- Relation[artist_id#2872,artist_latitude#2873,artist_location#2874,artist_longitude#2875,artist_name#2876,duration#2877,num_songs#2878L,song_id#2879,title#2880,year#2881L] json\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o20.sql.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`ts`' given input columns: [songplays.location, songplays.sessionid, songplays.useragent, songplays.start_time, songplays.level, songplays.song_id, songplays.artist_id, songplays.userid, songplays.year]; line 1 pos 7;\n'Project ['ts, cast(userid#2915 as int) AS userid#4601, level#2905, song_id#2879, artist_id#2872, sessionid#2910L, location#2906, useragent#2914]\n+- SubqueryAlias `songplays`\n   +- Project [start_time#3720, userid#2915, level#2905, song_id#2879, artist_id#2872, sessionid#2910L, location#2906, useragent#2914, year(cast(start_time#3720 as date)) AS year#3729]\n      +- Project [ts#3711 AS start_time#3720, userid#2915, level#2905, song_id#2879, artist_id#2872, sessionid#2910L, location#2906, useragent#2914]\n         +- Project [to_timestamp((ts#2913L / 1000), None) AS ts#3711, userid#2915, level#2905, song_id#2879, artist_id#2872, sessionid#2910L, location#2906, useragent#2914]\n            +- Project [ts#2913L, userid#2915, level#2905, song_id#2879, artist_id#2872, sessionid#2910L, location#2906, useragent#2914]\n               +- Filter ((page#2908 LIKE NextSong && isnotnull(song#2911)) && (isnotnull(artist_id#2872) && artist#2898 LIKE artist_name#2876))\n                  +- Join Inner, song#2911 LIKE title#2880\n                     :- SubqueryAlias `e`\n                     :  +- SubqueryAlias `staging_events`\n                     :     +- Relation[artist#2898,auth#2899,firstName#2900,gender#2901,itemInSession#2902L,lastName#2903,length#2904,level#2905,location#2906,method#2907,page#2908,registration#2909,sessionId#2910L,song#2911,status#2912L,ts#2913L,userAgent#2914,userId#2915] json\n                     +- SubqueryAlias `s`\n                        +- SubqueryAlias `staging_songs`\n                           +- Relation[artist_id#2872,artist_latitude#2873,artist_location#2874,artist_longitude#2875,artist_name#2876,duration#2877,num_songs#2878L,song_id#2879,title#2880,year#2881L] json\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:110)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:107)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:296)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:107)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n\tat sun.reflect.GeneratedMethodAccessor69.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-0962c6ae1292>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"time\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'select ts, int(userid), level, song_id, artist_id, sessionid, location, useragent from songplays'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;31m#sp_window = Window.partitionBy(\"year\").orderBy(\"age\")                 .rowsBetween(Window.unboundedPreceding, Window.currentRow)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0msongplays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msongplays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'start_time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_timestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstaging_events\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \"\"\"\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`ts`' given input columns: [songplays.location, songplays.sessionid, songplays.useragent, songplays.start_time, songplays.level, songplays.song_id, songplays.artist_id, songplays.userid, songplays.year]; line 1 pos 7;\\n'Project ['ts, cast(userid#2915 as int) AS userid#4601, level#2905, song_id#2879, artist_id#2872, sessionid#2910L, location#2906, useragent#2914]\\n+- SubqueryAlias `songplays`\\n   +- Project [start_time#3720, userid#2915, level#2905, song_id#2879, artist_id#2872, sessionid#2910L, location#2906, useragent#2914, year(cast(start_time#3720 as date)) AS year#3729]\\n      +- Project [ts#3711 AS start_time#3720, userid#2915, level#2905, song_id#2879, artist_id#2872, sessionid#2910L, location#2906, useragent#2914]\\n         +- Project [to_timestamp((ts#2913L / 1000), None) AS ts#3711, userid#2915, level#2905, song_id#2879, artist_id#2872, sessionid#2910L, location#2906, useragent#2914]\\n            +- Project [ts#2913L, userid#2915, level#2905, song_id#2879, artist_id#2872, sessionid#2910L, location#2906, useragent#2914]\\n               +- Filter ((page#2908 LIKE NextSong && isnotnull(song#2911)) && (isnotnull(artist_id#2872) && artist#2898 LIKE artist_name#2876))\\n                  +- Join Inner, song#2911 LIKE title#2880\\n                     :- SubqueryAlias `e`\\n                     :  +- SubqueryAlias `staging_events`\\n                     :     +- Relation[artist#2898,auth#2899,firstName#2900,gender#2901,itemInSession#2902L,lastName#2903,length#2904,level#2905,location#2906,method#2907,page#2908,registration#2909,sessionId#2910L,song#2911,status#2912L,ts#2913L,userAgent#2914,userId#2915] json\\n                     +- SubqueryAlias `s`\\n                        +- SubqueryAlias `staging_songs`\\n                           +- Relation[artist_id#2872,artist_latitude#2873,artist_location#2874,artist_longitude#2875,artist_name#2876,duration#2877,num_songs#2878L,song_id#2879,title#2880,year#2881L] json\\n\""
     ]
    }
   ],
   "source": [
    "### STEP 3 Define new schema for each table and apply\n",
    "\n",
    "###  >< Deprecated, do not use!\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, LongType, TimestampType\n",
    "from pyspark.sql import Window\n",
    "\n",
    "usr_schema = StructType([ \\\n",
    "    StructField(\"userid\",IntegerType(),False), \\\n",
    "    StructField(\"firstname\",StringType(),True), \\\n",
    "    StructField(\"lastname\",StringType(),True), \\\n",
    "    StructField(\"gender\", StringType(), True), \\\n",
    "    StructField(\"level\", StringType(), True), \\\n",
    "    ])\n",
    "\n",
    "song_schema = StructType([ \\\n",
    "    StructField(\"song_id\",StringType(),False), \\\n",
    "    StructField(\"title\",StringType(),True), \\\n",
    "    StructField(\"artist_id\",StringType(),True), \\\n",
    "    StructField(\"year\", IntegerType(), True), \\\n",
    "    StructField(\"duration\", FloatType(), True), \\\n",
    "    ])\n",
    "\n",
    "artist_schema = StructType([ \\\n",
    "    StructField(\"artist_id\",StringType(),False), \\\n",
    "    StructField(\"artist_name\",StringType(),False), \\\n",
    "    StructField(\"location\",StringType(),True), \\\n",
    "    StructField(\"latitude\", FloatType(), True), \\\n",
    "    StructField(\"longitude\", FloatType(), True), \\\n",
    "    ])\n",
    "\n",
    "time_schema = StructType([ \\\n",
    "    StructField(\"start_time\",TimestampType(),False), \\\n",
    "    StructField(\"hour\",IntegerType(),False), \\\n",
    "    StructField(\"day\",IntegerType(),False), \\\n",
    "    StructField(\"week\", IntegerType(), False), \\\n",
    "    StructField(\"month\", IntegerType(), False), \\\n",
    "    StructField(\"year\", IntegerType(), False), \\\n",
    "    StructField(\"dayofweek\", IntegerType(), False), \\\n",
    "    ])\n",
    "\n",
    "songplay_schema = StructType([ \\\n",
    "    StructField(\"songplay_id\",IntegerType(),False), \\\n",
    "    StructField(\"start_time\",TimestampType(),False), \\\n",
    "    StructField(\"user_id\", IntegerType(),False), \\\n",
    "    StructField(\"level\", StringType(), True), \\\n",
    "    StructField(\"song_id\", StringType(), False), \\\n",
    "    StructField(\"artist_id\", StringType(), False), \\\n",
    "    StructField(\"session_id\", StringType(), True), \\\n",
    "    StructField(\"location\", StringType(), True), \\\n",
    "    StructField(\"user_agent\", StringType(), True), \\\n",
    "    ])\n",
    "\n",
    "# Clean empty cells, remove duplicates and refresh views\n",
    "users = users.filter(col('userid') != '')\n",
    "users.createOrReplaceTempView(\"users\")\n",
    "content = spark.sql('select distinct int(userid), firstname, lastname, gender, level from users').collect()\n",
    "users = spark.createDataFrame(content,schema=usr_schema)\n",
    "users.createOrReplaceTempView(\"users\")\n",
    "\n",
    "content = spark.sql('select distinct * from songs').collect()\n",
    "songs = spark.createDataFrame(content,schema=song_schema)\n",
    "songs.createOrReplaceTempView(\"songs\")\n",
    "\n",
    "content = spark.sql('select distinct* from artists').collect()\n",
    "artists = spark.createDataFrame(content,schema=artist_schema)\n",
    "artists.createOrReplaceTempView(\"artists\")\n",
    "\n",
    "content = spark.sql('select distinct * from time').collect()\n",
    "time = spark.createDataFrame(content,schema=time_schema)\n",
    "time.createOrReplaceTempView(\"time\")\n",
    "\n",
    "content = spark.sql('select ts, int(userid), level, song_id, artist_id, sessionid, location, useragent from songplays').collect()\n",
    "#sp_window = Window.partitionBy(\"year\").orderBy(\"age\")                 .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "songplays = songplays.withColumn('start_time', to_timestamp(staging_events.ts/1000))\n",
    "songplays = songplays.withColumn('songplay_id', row_number().over(sp_window))\n",
    "songplays = songplays.select(songplay_columns)\n",
    "songplays.createOrReplaceTempView(\"songplays\")\n",
    "songplays = spark.createDataFrame(content,schema=songplay_schema)\n",
    "\n",
    "\n",
    "songplays.createOrReplaceTempView(\"songplays\")\n",
    "\n",
    "for frame in [songs, artists, users, time, songplays]:\n",
    "    print(frame.printSchema(), 'Number of entries: ', frame.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1547.showString.\n: java.lang.UnsupportedOperationException: Cannot evaluate expression: row_number()\n\tat org.apache.spark.sql.catalyst.expressions.Unevaluable$class.doGenCode(Expression.scala:261)\n\tat org.apache.spark.sql.catalyst.expressions.aggregate.DeclarativeAggregate.doGenCode(interfaces.scala:348)\n\tat org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:108)\n\tat org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:105)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:105)\n\tat org.apache.spark.sql.catalyst.expressions.Cast.doGenCode(Cast.scala:654)\n\tat org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:108)\n\tat org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:105)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:105)\n\tat org.apache.spark.sql.catalyst.expressions.Cast.genCode(Cast.scala:649)\n\tat org.apache.spark.sql.catalyst.expressions.Alias.genCode(namedExpressions.scala:155)\n\tat org.apache.spark.sql.execution.ProjectExec$$anonfun$6.apply(basicPhysicalOperators.scala:60)\n\tat org.apache.spark.sql.execution.ProjectExec$$anonfun$6.apply(basicPhysicalOperators.scala:60)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:60)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:189)\n\tat org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:374)\n\tat org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:403)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:374)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:45)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:35)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:544)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:598)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:339)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-ad51b5ce7ca2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msongplays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \"\"\"\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1547.showString.\n: java.lang.UnsupportedOperationException: Cannot evaluate expression: row_number()\n\tat org.apache.spark.sql.catalyst.expressions.Unevaluable$class.doGenCode(Expression.scala:261)\n\tat org.apache.spark.sql.catalyst.expressions.aggregate.DeclarativeAggregate.doGenCode(interfaces.scala:348)\n\tat org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:108)\n\tat org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:105)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:105)\n\tat org.apache.spark.sql.catalyst.expressions.Cast.doGenCode(Cast.scala:654)\n\tat org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:108)\n\tat org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:105)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:105)\n\tat org.apache.spark.sql.catalyst.expressions.Cast.genCode(Cast.scala:649)\n\tat org.apache.spark.sql.catalyst.expressions.Alias.genCode(namedExpressions.scala:155)\n\tat org.apache.spark.sql.execution.ProjectExec$$anonfun$6.apply(basicPhysicalOperators.scala:60)\n\tat org.apache.spark.sql.execution.ProjectExec$$anonfun$6.apply(basicPhysicalOperators.scala:60)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:60)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:189)\n\tat org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:374)\n\tat org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:403)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:374)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:45)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:35)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:544)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:598)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:339)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "songplays.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory for  songs  exists, no need to creae\n",
      "Directory for  users  exists, no need to creae\n",
      "Directory for  time  exists, no need to creae\n",
      "Directory for  artists  exists, no need to creae\n",
      "Directory for  songplays  exists, no need to creae\n",
      "Parquet files written\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "s3_pref = ''\n",
    "\n",
    "# Create directory structure if not exists:\n",
    "try:\n",
    "    os.chdir('/home/workspace')\n",
    "except:\n",
    "    print('No workspace root found')\n",
    "\n",
    "for table in analytics_tables:\n",
    "    if table in os.listdir():\n",
    "        print('Directory for ', table, ' exists, no need to creae')\n",
    "    else:\n",
    "        print('Directory not found for table ', table, '. Creating now')\n",
    "        os.mkdir(table)\n",
    "\n",
    "prefix = \"/home/workspace/\"\n",
    "#users.write.parquet('/home/workspace/users/users.parquet', mode='overwrite', compression='gzip')\n",
    "users.write.parquet((prefix + 'users/users.parquet'), mode='overwrite', compression='gzip')\n",
    "songs.write.parquet('/home/workspace/songs/songs.parquet', mode='overwrite', compression='gzip', partitionBy=['year', 'artist_id'])\n",
    "artists.write.parquet('/home/workspace/artists/artists.parquet', mode='overwrite', compression='gzip')\n",
    "time.write.parquet('/home/workspace/time/time.parquet', mode='overwrite', compression='gzip', partitionBy=['year', 'month'])\n",
    "songplays = songplays.withColumn('year', year(songplays.start_time)).withColumn('month', month(songplays.start_time))\n",
    "songplays.write.parquet('/home/workspace/songplays/songplays.parquet', mode='overwrite', compression='gzip', partitionBy=['year', 'month'])\n",
    "print('Parquet files written')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-02 12:09:49.688066 : Setting up resource for  s3\n",
      "yes\n"
     ]
    },
    {
     "ename": "BucketAlreadyOwnedByYou",
     "evalue": "An error occurred (BucketAlreadyOwnedByYou) when calling the CreateBucket operation: Your previous request to create the named bucket succeeded and you already own it.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBucketAlreadyOwnedByYou\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-5dd10737b89e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbucket_name\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_of_buckets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Buckets'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mlocation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'LocationConstraint'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'us-west-2'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0ms3_buk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms3_cli\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_bucket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbucket_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCreateBucketConfiguration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    319\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    621\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBucketAlreadyOwnedByYou\u001b[0m: An error occurred (BucketAlreadyOwnedByYou) when calling the CreateBucket operation: Your previous request to create the named bucket succeeded and you already own it."
     ]
    }
   ],
   "source": [
    "import botocore\n",
    "import boto3\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "AWS_ACCESS_KEY_ID = config.get('AWS', 'AWS_ACCESS_KEY_ID')\n",
    "AWS_SECRET_ACCESS_KEY = config.get('AWS', 'AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "bucket_name = 'udal4p1results'\n",
    "\n",
    "def create_resource(KEY, SECRET, TYPE):\n",
    "    \"\"\" Create a resource for AWS specified as in TYPE (e.g. S3)\n",
    "        Returns: AWS Resource\"\"\"\n",
    "    try:\n",
    "        print(datetime.now(), ': Setting up resource for ', TYPE) \n",
    "        aws_cli = boto3.client(TYPE,\n",
    "                               region_name='us-west-2',\n",
    "                               aws_access_key_id=KEY,\n",
    "                               aws_secret_access_key=SECRET\n",
    "                               )\n",
    "    except Exception as e:\n",
    "        print(datetime.now(), ': FAILED creating resource: ', e)\n",
    "    return aws_cli\n",
    "\n",
    "s3_cli = create_resource(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, 's3')\n",
    "list_of_buckets = s3_cli.list_buckets()\n",
    "for bucket in list_of_buckets['Buckets']:\n",
    "    if bucket_name in bucket.get('Name'): print('yes')\n",
    "\n",
    "if bucket_name not in list_of_buckets['Buckets']:\n",
    "    location = {'LocationConstraint': 'us-west-2'}\n",
    "    s3_buk = s3_cli.create_bucket(Bucket=bucket_name, CreateBucketConfiguration=location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
